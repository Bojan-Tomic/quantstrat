% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/haircut.Sharpe.R
\name{haircutSharpe}
\alias{haircutSharpe}
\alias{SharpeRatio.haircut}
\alias{.haircutSR}
\title{Haircut Sharpe Ratio to correct for number of trials and autocorrelation}
\usage{
haircutSharpe(portfolios, ..., strategy = NULL, trials = NULL,
  audit = NULL, env = .GlobalEnv)

.haircutSR(sm_fre, num_obs, SR, ind_an, ind_aut, rho, num_test, RHO)
}
\arguments{
\item{portfolios}{string name of portfolio, or optionally a vector of portfolios, see DETAILS}

\item{...}{any other passtrhrough parameters}

\item{strategy}{optional strategy specification that would contain more information on the process, default NULL}

\item{trials}{optional number of trials,default NULL}

\item{audit}{optional audit environment containing the results of parameter optimization or walk forward, default NULL}

\item{env}{optional environment to find market data in, if required.}

\item{sm_fre}{Sampling frequency; [1,2,3,4,5] = [Daily, Weekly, Monthly, Quarterly, Annual]}

\item{num_obs}{No. of observations in the frequency specified in the previous step}

\item{SR}{Sharpe ratio; either annualized or in the frequency specified in the previous step}

\item{ind_an}{Indicator; if annulized, 'ind_an' = 1; otherwise = 0}

\item{ind_aut}{Indicator; if adjusted for autocorrelations, 'ind_aut' = 0; otherwise = 1}

\item{rho}{Autocorrelation coefficient at the specified frequency}

\item{num_test}{'num_test': Number of tests allowed. e.g. Harvey, Liu and Zhu (2014) find 315 published equity risk factors}

\item{RHO}{Average correlation among contemporaneous strategy returns}
}
\value{
an object of type \code{haircutSR} containing:

\describe{
   \item{Bonferroni}{a \code{data.frame} containing slots \code{haircut_SR},\code{adj_pvalue},and \code{pct_adj}}
   \item{Holm}{a \code{data.frame} containing slots \code{haircut_SR},\code{adj_pvalue},and \code{pct_adj}}
   \item{BHY}{a \code{data.frame} containing slots \code{haircut_SR},\code{adj_pvalue},and \code{pct_adj}}
   \item{average}{a \code{data.frame} containing slots \code{haircut_SR},\code{adj_pvalue},and \code{pct_adj}}
   \item{freq}{output frequency}
   \item{sm_fre}{Sampling frequency; [1,2,3,4,5] = [Daily, Weekly, Monthly, Quarterly, Annual]}
   \item{num_obs}{No. of observations in the frequency specified in the previous step}
   \item{SR}{observed Sharpe Ratio}
   \item{SR_ac_adj}{Observed Sharpe Ratio corrected for autocorrelation}
   \item{ind_an}{Indicator; if annulized, 'ind_an' = 1; otherwise = 0}
   \item{ind_aut}{Indicator; if adjusted for autocorrelations, 'ind_aut' = 0; otherwise = 1}
   \item{rho}{Autocorrelation coefficient at the specified frequency}
   \item{num_trials}{number or trials}
   \item{RHO}{Average correlation among contemporaneous strategy returns}
   \item{call}{call used for \code{\link{SharpeRatio.haircut}}}
   \item{inner_call}{call used to call \code{\link{.haircutSR}}}
}
}
\description{
In their 2015 JPM paper "Backtesting", Campbell Harvey and Yan Liu (HL) discuss
the common practice of adjusting or 'haircutting' a set of backtest results
to correct for assumed overfitting.  In the industry, this haircut is often 
assumed to be 50\% of reported performance.  They propose and demonstrate
three methods of adjusting for potential multiple testing bias by adapting 
three methods for adjusting confidence on multiple trials from the statstical
literature.

The non-exported \code{.haircutSR} function is the internal implementation of
the haircut Sharpe Ratio calculations. It is based off of the code  written 
by Jasen Mackie for his blog post referenced below. It calcualtes an equivalent
adjusted Sharpe Ratio after taking the number and autocorrelation of the
trials into account.
}
\details{
In multiple hypothesis testing the challenge is to guard against false 
discoveries. HL argue that the appropriate "haircut Sharpe ratio" is
non-linear, in that the highest Sharpe ratios (SR's) are only moderately
penalized whilst marginal SR's more so. The implication is that high SR's
are more likely true discoveries in a multiple hypothesis testing framework.

HL mention 5 caveats to their framework, namely;

\itemize{
 \item{Sharpe ratios may not be appropriate metrics for strategies with negatively skewed expected payoffs, such as option strategies.}
 \item{Sharpe ratios normalize returns based on their volatility (ie. market risk), which may not be the most appropriate reflection of risk for a strategy.}
 \item{Determining the appropriate significance level for multiple testing (where in single tests 5\% is the normal cutoff).}
 \item{Which multiple testing method you choose could yield different conclusions. HL proposes 3 methods together with an average.}     
 \item{The number of trials used to adjust for multiple tests}
}
}
\section{Linking Sharpe ratio with t-statistic}{


To explain the link between the Sharpe ratio and the t-stat and the application 
of a multiple testing p-value adjustment, HL use the simplest case of an 
individual investment strategy. Assume a null hypothesis in which the 
strategy's mean return is significantly different from zero, therefore 
implying a 2-sided alternative hypothesis. A strategy can be regarded as 
profitable if its mean returns are either side of zero since investors can 
generally go long or short. Since returns will at least be asymptotically 
normally distributed (thanks to the Central Limit Theorem) a t-statistic 
following a t-distribution can be constructed and tested for significance. 
Due to the link between the Sharpe ratio and the t-stat it is possible to 
assess the significance of a strategy's excess returns directly using the 
Sharpe ratio. Assume \eqn{\hat{\mu}}{mu} denotes the mean of your sample of 
historical returns (daily or weekly etc) and \eqn{\hat{\sigma}}{sigma} denotes 
standard deviation, then:

\deqn{t-statistic = \frac{ \hat{\mu}}{(\frac{\hat{\sigma}}{\sqrt{T})}}}{tstatistic = mu/(sigma/sqrt(T))}

where \eqn{T-1} is degrees of freedom and since

\deqn{\widehat{SR} =  \frac{\hat{\mu}}{\hat{\sigma}}}{SR = mu/sigma}

it may be shown that

\deqn{\widehat{SR} = \frac{t-statistic}{\sqrt{T}}}{SR = tstatistic/sqrt(T)}

By implication a higher Sharpe ratio equates to a higher t-ratio, implying a 
higher significance level (lower p-value) for an investment strategy. If we 
denote p-value of the single test as \eqn{p^s} then we can present the p-value
for a single test as:

\deqn{{p^s} = Pr( |r| > t-ratio)}{p^s = Pr(|r|>t-ratio)}

or

\deqn{{p^s} = Pr( |r| > \widehat{SR}.\sqrt{T}}{p^s = Pr( |r| > SR * sqrt{T}}

If the researcher was exploring a particular economic theory then this p-value 
might make sense, but what if the researcher has tested multiple strategies 
and presents only the most profitable one? In this case the p-value of the 
single test may severely overstate the actual significance. A more truthful 
p-value would be an adjusted multiple testing p-value which assuming we denote 
as \eqn{p^m} which could be represented as:

\deqn{{p^m} = Pr( max{|{r_i}|, i = 1,...,N} > t-ratio)}{p^m = Pr( max(|r_i|, i = 1,...,N) > t-ratio)}

or

\deqn{ {p^m} = 1 - (1 - {p^s}{)^N} }{ p^m = 1 - (1 -p^s)^N }

By equating the p-value of a single test to a multiple test p-value we get 
the defining equation of \eqn{p^m} which is

\deqn{  {p^m} = Pr( {|{r_i}|} > \widehat{SR}.\sqrt{T})  }{ p^m = Pr(|r_i| > SR * sqrt(T))  }

where

\deqn{ {p^m} = 1 - (1 - {p^s}{)^N} }{p^m = 1 - (1-p^s)^N }
}

\section{Multiple Testing Methods}{


This function replicates the methods proposed by Harvey and Liu to adjust
an observed Sharpe Ratio for the number of trials performed, the
autocorrelation between the trials, the overall level of performance, and 
the presumed or observed correlation between trials.

We will refer to these methods as:

1. Bonferroni (BON)

2. Holm

3. Benjamini, Hochberg and Yekutieli (BHY)

Full details on the calculations and adjustments should be found in 
Harvey and Liu (2015).  This documentation is just an overview to aid in 
easy use of the \R function.

HL mention 3 well known adjustment methods in the statistics literature, which
are originally prescribed in the paper "...and the Cross-Section of Expected
Returns" by Harvey, Liu and Zhu. These are Bonferroni, Holm, and Benjamini,
Hochberg, and Yekutieli (BHY).

1. Bonferroni (BON)

\deqn{p^Bonferroni = min {|{M * p_i, 1}|}}{p^Bonferroni=min(|M*p_1,1|)}

Bonferroni applies the same adjustment to the p-value of each test, inflating
the p-value by the number of tests. The multiple testing p-value is the minimum
of each inflated p-value and 1 where 1 (or 100% if you prefer) is the upper bound
of probability. HL use the example of p-values from 6 strategies where the p-values
are (0.005, 0.009, 0.0128, 0.0135, 0.045, 0.06). According to a 5% significance
cutoff the first 5 tests would be considered significant. Using the p.adjust function
in R we can get the multiple adjusted p-values and according to Bonferroni only the
first test would be considered significant.

2. Holm

p-value adjustments can be categorized into 2 categories, namely: single-step and
sequential. Single-step corrections equally adjust p-values as in Bonferroni.
Sequential adjustments are an adaptive procedure based on the distribution of p-values.
Sequential methods gained prominence after a seminal paper by Schweder & Spjotvoll (1982)
and section 7.3 of this paper gives a useful example of an application of multiple
testing hypothesis diagnostic plotting in R. Holm is an example of a sequential
multiple testing procedure. For Holm, the equivalent adjusted p-value is

\deqn{{p_{(i)}}^Holm = min[max((M - j + 1)*{p_{(j)}} ),1]}{p_i^Holm=min[max((M-j+1)*p_j),1]}

Bonferroni adjusts single tests equally, whereas Holm applies a sequential approach.
By conclusion it should not surprise you that adjusted Sharpe ratios under Bonferroni
will therefore be lower than for Holm. At this point it is useful to mention that both
Holm and Bonferroni attempt to prevent even 1 Type I error occurring, controlling what
is called the family-wise error rate (FWER). The next adjustment proposed by HL is BHY
and the main difference from the previous 2 adjustment methods is that BHY attempts to
control the false discovery rate (FDR), implying more lenience than Holm and Bonferroni
and therefore expected to yield higher adjusted Sharpe ratios.

3. BHY

BHY's formulation of the FDR can be represented as follows. First all p-values are
sorted in descending order and the adjusted p-value sequence is defined by pairwise
comparisons.

TODO: BHY equation

We expect BHY to be more lenient as it controls the false discovery rate whereas Holm
and Bonferroni control the family-wise error rate, trying to eliminate making even 1
false discovery. Bonferroni is more stringent than Holm since it is a single-step
adjustment versus the sequential approach of Holm. With these 3 methods HL attempt to
adjust p-values to account for multiple testing and then convert these to haircut Sharpe
ratios and in so doing control for data mining. For both Holm and BHY you need the
empirical distribution of p-values from previously tried strategies.
Harvey, Liu and Zhu (HLZ) model over 300 risk factors documented in the finance literature.
However, using this model for the distribution of p-values is not complete since many tried
strategies would not have been documented (referred to as Publication Bias) plus they
are potentially correlated thereby violating the requirement for independence between tests.
HLZ propose a new distribution to overcome these shortfalls.
}

\references{
Harvey, Campbell R. and Yan Liu. 2015. Backtesting The Journal of Portfolio Management. 41:1 pp. 13-28. 

Harvey, Campbell R., Yan Liu, and Heqing Zhu. "… and the cross-section of expected returns." The Review of Financial Studies 29, no. 1 (2016): 5-68.

Mackie, Jasen. 2016. R-view: Backtesting - Harvey & Liu (2015). https://opensourcequant.wordpress.com/2016/11/17/r-view-backtesting-harvey-liu-2015/
}
\seealso{
\code{\link{SharpeRatio.deflated}}
}
\author{
Jasen Mackie, Brian G. Peterson
}
